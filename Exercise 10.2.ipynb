{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf052f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7512/2906576411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DSC 400 Assignment 10\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DSC 400 Assignment 10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# TODO: Change these to point to a version on your local path\n",
    "sample_libsvm_data_path = (r'C:\\Users\\12162\\Documents\\BU\\Winter 2022\\DSC 400\\Week 10\\sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64242858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12162\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7512/3265068793.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'libsvm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_libsvm_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# TODO: Implement the remainder of the code from the binomial logistic regression example example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format('libsvm').load(sample_libsvm_data_path)\n",
    "\n",
    "# TODO: Implement the remainder of the code from the binomial logistic regression example example\n",
    "#________________________________________________________________________________________________\n",
    "\n",
    "# Code to be implimented:\n",
    "# _______________________\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print('Coefficients: ' + str(lrModel.coefficients))\n",
    "print('Intercept: ' + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family='multinomial')\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print('Multinomial coefficients: ' + str(mlrModel.coefficientMatrix))\n",
    "print('Multinomial intercepts: ' + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f821508e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7512/1807669993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extract the summary from the returned LogisticRegressionModel instance trained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# in the earlier example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainingSummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Obtain the objective per iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lrModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print('objectiveHistory:')\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a df and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print('areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1434ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "file_url = 'http://storage.googleapis.com/download.tensorflow.org/data/heart.csv'\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "# TODO: Complete the remainder of the example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d14217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f23643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Val sizes: (242, 61)\n"
     ]
    }
   ],
   "source": [
    "# Creates dfs for training and validating the model\n",
    "val_df = df.sample(frac=0.2, random_state=1337)\n",
    "train_df = df.drop(val_df.index)\n",
    "\n",
    "print(f'Train, Val sizes: {len(train_df), len(val_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbdd63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates TF dataset objects from dataframes\n",
    "def df_to_ds(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    labels = df.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(df))\n",
    "    return ds\n",
    "\n",
    "# Creates TF ds objects from dfs\n",
    "train_ds = df_to_ds(train_df)\n",
    "val_ds = df_to_ds(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e90e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches the ds - Adds dimension to shape\n",
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0356b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "\n",
    "# Normoralizes and engineers features quantitative\n",
    "def encode_numerical_feature(feature, name, ds):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a dataset that only yields our feature\n",
    "    feature_ds = ds.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "# Normoralizes and engineers features qualitative\n",
    "def encode_categorical_feature(feature, name, ds, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode='binary')\n",
    "\n",
    "    # Prepare a ds that only yields our feature\n",
    "    feature_ds = ds.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e651665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizes Data by type\n",
    "data_cats = {\n",
    "    'continious': ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope'],\n",
    "    'discrete': [ 'sex', 'cp', 'fbs', 'restecg', 'exang', 'ca'],\n",
    "    'string': ['thal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a46c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\12162\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# The following code was written take up much less code-block realestate\n",
    "# ...than it does in the tutorial. It also supports easier adjustments to\n",
    "# ...input data now via editing data_cats{} values.\n",
    "# ______________________________________________________________________\n",
    "\n",
    "all_inputs = []\n",
    "all_features = []\n",
    "\n",
    "# Encodes numerically continious features\n",
    "for label in data_cats.get('continious'):\n",
    "    feat_input = keras.Input(shape=(1,), name=label)\n",
    "    feature = encode_numerical_feature(feat_input, label, train_ds)\n",
    "    \n",
    "    all_inputs.append(feat_input)\n",
    "    all_features.append(feature)\n",
    "\n",
    "# Encodes numerically discrete features\n",
    "for label in data_cats.get('discrete'):\n",
    "    feat_input = keras.Input(shape=(1,), name=label, dtype='int64')\n",
    "    feature = encode_categorical_feature(feat_input, label, train_ds, False)\n",
    "    \n",
    "    all_inputs.append(feat_input)\n",
    "    all_features.append(feature)\n",
    "\n",
    "# Encodes string features\n",
    "for label in data_cats.get('string'):\n",
    "    feat_input = keras.Input(shape=(1,), name=label, dtype='string')\n",
    "    feature = encode_categorical_feature(feat_input, label, train_ds, True)\n",
    "    \n",
    "    all_inputs.append(feat_input)\n",
    "    all_features.append(feature)\n",
    "\n",
    "all_features = layers.concatenate(all_features)\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(all_inputs, output)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf370eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 0.8215 - accuracy: 0.4174 - val_loss: 0.7150 - val_accuracy: 0.4754\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7360 - accuracy: 0.5248 - val_loss: 0.6576 - val_accuracy: 0.5902\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.6074 - val_loss: 0.6121 - val_accuracy: 0.6557\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5939 - accuracy: 0.6818 - val_loss: 0.5726 - val_accuracy: 0.7049\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5801 - accuracy: 0.7107 - val_loss: 0.5404 - val_accuracy: 0.7377\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5647 - accuracy: 0.7107 - val_loss: 0.5141 - val_accuracy: 0.7377\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5470 - accuracy: 0.7190 - val_loss: 0.4917 - val_accuracy: 0.7377\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5332 - accuracy: 0.7231 - val_loss: 0.4732 - val_accuracy: 0.7049\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4876 - accuracy: 0.7810 - val_loss: 0.4577 - val_accuracy: 0.7377\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4731 - accuracy: 0.7645 - val_loss: 0.4431 - val_accuracy: 0.7541\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4605 - accuracy: 0.7686 - val_loss: 0.4316 - val_accuracy: 0.7541\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.7975 - val_loss: 0.4219 - val_accuracy: 0.7869\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4509 - accuracy: 0.7686 - val_loss: 0.4132 - val_accuracy: 0.7869\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 0.8182 - val_loss: 0.4054 - val_accuracy: 0.7869\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4138 - accuracy: 0.8099 - val_loss: 0.3988 - val_accuracy: 0.7869\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4235 - accuracy: 0.7893 - val_loss: 0.3930 - val_accuracy: 0.7869\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3739 - accuracy: 0.8264 - val_loss: 0.3886 - val_accuracy: 0.7869\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4135 - accuracy: 0.7851 - val_loss: 0.3850 - val_accuracy: 0.7869\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3687 - accuracy: 0.8182 - val_loss: 0.3818 - val_accuracy: 0.7869\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8182 - val_loss: 0.3795 - val_accuracy: 0.8033\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8471 - val_loss: 0.3774 - val_accuracy: 0.8197\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8140 - val_loss: 0.3761 - val_accuracy: 0.8361\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.8595 - val_loss: 0.3752 - val_accuracy: 0.8361\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8347 - val_loss: 0.3746 - val_accuracy: 0.8361\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3699 - accuracy: 0.8264 - val_loss: 0.3742 - val_accuracy: 0.8361\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3434 - accuracy: 0.8636 - val_loss: 0.3735 - val_accuracy: 0.8361\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3493 - accuracy: 0.8430 - val_loss: 0.3727 - val_accuracy: 0.8361\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8388 - val_loss: 0.3728 - val_accuracy: 0.8361\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3505 - accuracy: 0.8471 - val_loss: 0.3722 - val_accuracy: 0.8361\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3424 - accuracy: 0.8388 - val_loss: 0.3719 - val_accuracy: 0.8361\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2964 - accuracy: 0.8719 - val_loss: 0.3720 - val_accuracy: 0.8361\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.8554 - val_loss: 0.3730 - val_accuracy: 0.8361\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3272 - accuracy: 0.8595 - val_loss: 0.3732 - val_accuracy: 0.8361\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3108 - accuracy: 0.8802 - val_loss: 0.3739 - val_accuracy: 0.8197\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3088 - accuracy: 0.8595 - val_loss: 0.3742 - val_accuracy: 0.8197\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8595 - val_loss: 0.3745 - val_accuracy: 0.8197\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8636 - val_loss: 0.3742 - val_accuracy: 0.8361\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8554 - val_loss: 0.3746 - val_accuracy: 0.8361\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8595 - val_loss: 0.3752 - val_accuracy: 0.8361\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3241 - accuracy: 0.8554 - val_loss: 0.3757 - val_accuracy: 0.8361\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8554 - val_loss: 0.3760 - val_accuracy: 0.8197\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3015 - accuracy: 0.8678 - val_loss: 0.3762 - val_accuracy: 0.8197\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2881 - accuracy: 0.8760 - val_loss: 0.3762 - val_accuracy: 0.8361\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2957 - accuracy: 0.8843 - val_loss: 0.3768 - val_accuracy: 0.8361\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3087 - accuracy: 0.8595 - val_loss: 0.3771 - val_accuracy: 0.8361\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2928 - accuracy: 0.8884 - val_loss: 0.3771 - val_accuracy: 0.8361\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2955 - accuracy: 0.8802 - val_loss: 0.3777 - val_accuracy: 0.8361\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.8719 - val_loss: 0.3780 - val_accuracy: 0.8361\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2974 - accuracy: 0.8554 - val_loss: 0.3786 - val_accuracy: 0.8361\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2762 - accuracy: 0.8719 - val_loss: 0.3786 - val_accuracy: 0.8361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e601cab100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57f46d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 296ms/step\n",
      "This particular patient had a 34.514403343200684 percent probability of having a heart disease, as evaluated by the deep learning model.\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    'age': 60,\n",
    "    'sex': 1,\n",
    "    'cp': 1,\n",
    "    'trestbps': 145,\n",
    "    'chol': 233,\n",
    "    'fbs': 1,\n",
    "    'restecg': 2,\n",
    "    'thalach': 150,\n",
    "    'exang': 0,\n",
    "    'oldpeak': 2.3,\n",
    "    'slope': 3,\n",
    "    'ca': 0,\n",
    "    'thal': 'fixed',\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "preds = model.predict(input_dict)\n",
    "\n",
    "print(\n",
    "    f'This particular patient had a {100 * preds[0][0]} percent probability '\n",
    "    'of having a heart disease, as evaluated by the deep learning model.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7bb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
